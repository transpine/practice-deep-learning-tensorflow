{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)= tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train.astype('float32'), x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255., x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "input_size = 784\n",
    "hidden1_size = 256\n",
    "hidden2_size = 256\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.shuffle(60000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_normal_initializer_with_stddev_1():\n",
    "  return tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(ANN, self).__init__()\n",
    "    self.hidden_layer_1 = tf.keras.layers.Dense(hidden1_size,\n",
    "                                                activation='relu', \n",
    "                                                kernel_initializer=random_normal_initializer_with_stddev_1(),\n",
    "                                                bias_initializer=random_normal_initializer_with_stddev_1())\n",
    "    self.hidden_layer_2 = tf.keras.layers.Dense(hidden2_size,\n",
    "                                                activation='relu', \n",
    "                                                kernel_initializer=random_normal_initializer_with_stddev_1(),\n",
    "                                                bias_initializer=random_normal_initializer_with_stddev_1())\n",
    "    self.output_layer = tf.keras.layers.Dense(output_size,\n",
    "                                                activation=None,\n",
    "                                                kernel_initializer=random_normal_initializer_with_stddev_1(), \n",
    "                                                bias_initializer=random_normal_initializer_with_stddev_1() )\n",
    "\n",
    "  def call(self, x):\n",
    "    H1_output = self.hidden_layer_1(x)\n",
    "    H2_output = self.hidden_layer_2(H1_output)\n",
    "    logits = self.output_layer(H2_output)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def cross_entropy_loss(logits, y):\n",
    "  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_pred = model(x)\n",
    "    loss = cross_entropy_loss(y_pred, y)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_accuracy(y_pred, y):\n",
    "  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_model = ANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복: 1, 손실: 211.122955\n",
      "반복: 2, 손실: 57.533615\n",
      "반복: 3, 손실: 37.536491\n",
      "반복: 4, 손실: 27.683788\n",
      "반복: 5, 손실: 21.439924\n",
      "반복: 6, 손실: 17.162769\n",
      "반복: 7, 손실: 13.837636\n",
      "반복: 8, 손실: 11.334475\n",
      "반복: 9, 손실: 9.230560\n",
      "반복: 10, 손실: 7.524875\n",
      "반복: 11, 손실: 6.210339\n",
      "반복: 12, 손실: 5.080812\n",
      "반복: 13, 손실: 4.105843\n",
      "반복: 14, 손실: 3.361346\n",
      "반복: 15, 손실: 2.684852\n",
      "반복: 16, 손실: 2.162977\n",
      "반복: 17, 손실: 1.751295\n",
      "반복: 18, 손실: 1.357668\n",
      "반복: 19, 손실: 1.066137\n",
      "반복: 20, 손실: 0.816950\n",
      "반복: 21, 손실: 0.631774\n",
      "반복: 22, 손실: 0.504491\n",
      "반복: 23, 손실: 0.350409\n",
      "반복: 24, 손실: 0.304011\n",
      "반복: 25, 손실: 0.249523\n",
      "반복: 26, 손실: 0.153356\n",
      "반복: 27, 손실: 0.126595\n",
      "반복: 28, 손실: 0.135526\n",
      "반복: 29, 손실: 0.118347\n",
      "반복: 30, 손실: 0.086562\n",
      "정확도 : 0.944300\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "  average_loss = 0.\n",
    "  total_batch = int( x_train.shape[0] / batch_size )\n",
    "\n",
    "  for batch_x, batch_y in train_data:\n",
    "    _, current_loss = train_step(ANN_model, batch_x, batch_y), cross_entropy_loss(ANN_model(batch_x), batch_y)\n",
    "    average_loss += current_loss / total_batch\n",
    "  \n",
    "  if epoch % display_step == 0:\n",
    "    print(\"반복: %d, 손실: %f\" % ((epoch+1), average_loss ))\n",
    "\n",
    "print(\"정확도 : %f\" % compute_accuracy(ANN_model(x_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f21a28907bd242e258084ea54ba990dcd1f4be497b2c8a0510eed57636297e3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('.env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
